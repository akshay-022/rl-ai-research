Approaches to Long-Term Memory in Large Language Models
Large Language Models (LLMs) are notoriously limited to a short “working memory” – essentially the tokens in their context window. They do not automatically store new information beyond the current prompt. This poses a challenge for long-term coherence, learning new knowledge, or maintaining persona/contexts across sessions. Researchers have pursued many approaches to give LLMs long-term memory, broadly falling into two categories: using external context or memory structures (non-parametric memory) and incorporating knowledge into the model’s own weights (parametric memory). Below is a comprehensive survey of these approaches, along with their efficacy as reported in recent literature (mostly 2023–2025).
Extended Context Windows and Long-Context Models
One straightforward way to enable longer memory is to increase the model’s context window so it can attend to more past information directly. Recent LLMs have pushed context lengths from the classic 2k–4k tokens to tens or even hundreds of thousands:
Large Context Proprietary Models: Anthropic’s Claude 2 offers up to a 100k-token window, and OpenAI’s GPT-4 has 32k and even 128k-token variants[1]. Google’s Gemini 1.5 (2024) achieved an unprecedented 1 million token context in a prototype, meaning it can process entire books or codebases in one prompt[2][3]. In fact, Gemini 1.5 was tested on finding a “needle in a haystack” – locating a small relevant snippet buried in a huge text – and succeeded 99% of the time even with 1 million-token inputs[4]. This indicates that, at least for retrieval-style tasks, extremely long context can be utilized effectively.
Efficient Attention & Segment Processing: To cope with the quadratic cost of standard attention on long inputs, researchers developed efficient transformer variants (“x-formers”) like Longformer, BigBird, etc., that use sparse or local attention. More recently, the Recurrent Memory Transformer (RMT) adds a recurrent state to carry information between segments, enabling effectively unbounded sequences. RMT demonstrated processing of up to 2 million tokens by splitting text into segments and using a learned memory vector to shuttle info between segments[5]. Notably, as more segments are fed, language modeling perplexity improves, showing the model is leveraging the extended context[5]. In other words, RMT can remember earlier segments to predict later text better. This linear-scaling approach broke previous context length limits (even running on a single GPU) while maintaining high retrieval accuracy over long text[5].
Limitations (“Context Rot”): Simply having a huge context window doesn’t guarantee the model truly uses all that information effectively. Researchers observed a “context rot” effect: as more tokens flood the context, models start to lose focus or give diminishing returns[6][7]. Every new token consumes part of the model’s finite “attention budget,” and beyond a point the model may effectively ignore or forget earlier tokens[8]. Anthropic noted that LLMs’ ability to utilize context decays as context length grows, due to the transformer’s attention having to spread over many tokens[9]. Empirical studies like Lost in the Middle (2023) found models often overweight the beginning and end of a long prompt, failing to fully use the middle portions. These issues mean naïvely extending context has limits. Nonetheless, engineering tricks (e.g. positioning important info at the end of the prompt) and architectural innovations are mitigating this. For example, Google’s long-context models claim to maintain high performance even at 100k+ tokens[4], thanks to optimized attention algorithms and training procedures.
Efficacy: Extended context helps in scenarios like reading long documents, code, or multi-turn dialogues within a single session. We now have vivid demonstrations: Claude can summarize or answer questions about a novel in one go, and Gemini 1.5 could reason over the entire 402-page Apollo mission transcript in its prompt[10]. The Needle in a Haystack test mentioned above shows a model retrieving a factual detail buried in 1M tokens with 99% success[11]. However, beyond such retrieval or summary tasks, maintaining coherent reasoning over extremely long contexts remains challenging. Researchers suspect that truly long-horizon reasoning will require new architectures or memory mechanisms beyond brute-force attention[12]. In sum, very large context windows are a powerful form of short-term memory extension, but they face scaling challenges (computational cost grows, and attention focus fades). This motivates other approaches below.
Retrieval-Augmented Memory (External Knowledge Stores)
Another major strategy is to equip LLMs with an external knowledge base or memory store and fetch information on the fly. This falls under Retrieval-Augmented Generation (RAG), where the model is no longer limited to what’s in its fixed weights or immediate context – it can query a database of documents, past dialogues, or facts when needed.
How it works: The system stores a corpus of text (or conversation history) in an embedding vector database. When the LLM needs to recall something (e.g. a user’s earlier question, or a fact about the world), it converts the query into an embedding and finds semantically similar pieces of text in the store. These retrieved chunks are then inserted into the LLM’s context (prompt) as additional context for generation[13][14]. Essentially, the LLM “refreshes” its context by pulling in relevant external text as needed, giving it a form of long-term memory without increasing the core model size.
Improved Knowledge and Accuracy: Retrieval augmentation has proven very effective for factual and knowledge-intensive tasks. For example, DeepMind’s RETRO model (2022) showed that a relatively small model (7.5B parameters) augmented with a huge text index can outperform models 25× larger that have no retrieval[15]. Specifically, RETRO (with a database of trillions of tokens) outperformed a 175B model (Jurassic-1) on many language benchmarks[15] by retrieving relevant training passages and conditioning on them. The retrieved text helps the model produce more accurate and on-topic continuations[16][15], reducing hallucinations. Similarly, Meta’s Atlas (2022) and other open-domain QA systems found that tying an LLM to Wikipedia or the web via search significantly boosts factual question-answering performance compared to a closed-book model. In essence, retrieval acts as a large external memory that the model can consult as needed, rather than hoping the needed fact was memorized in its weights.
Memory of Conversations (Chat History): In dialogue systems, retrieval can be used to remember past conversations. Instead of keeping the entire chat history in the prompt (which eventually exceeds the context window), many implementations store older dialogue turns in a vector store and retrieve the most relevant past snippets when the user says something related. Open-source frameworks like LangChain, GPTIndex, and others have built-in “conversation memory” modules that do exactly this[17]. They embed each message or summary and on each new user query, fetch the top relevant past messages to include. This allows a chatbot to retain continuity over unlimited conversations – effectively a long-term episodic memory – without running out of context space.
Real-World Use: Products like Bing Chat and some ChatGPT plugins use web search (a form of retrieval) to fetch up-to-date info. More recently, OpenAI’s own ChatGPT gained a “Memory” feature that references all your past conversations for personalization[18]. Under the hood, this likely involves embedding past chats and retrieving insights or facts about the user when generating responses. In fact, ChatGPT now blends episodic memory (specific past interactions) with semantic memory (general facts learned about the user)[19]. For example, if weeks ago you told it that your favorite food is sushi, it might recall that preference in a later session without you restating it – because it retrieved that detail from stored memory. This marks a shift from early stateless chatbots to ones that continuously learn about the user.
Challenges: The efficacy of retrieval depends heavily on the quality of the retriever. If the wrong documents are fetched, the model’s answer may be irrelevant or incorrect (garbage in, garbage out). Training good retrievers or using hybrid search (combining keyword and embedding search) can mitigate this. Another issue is that the model might over-rely on retrieved text (just copying it) or misattribute sources. Nonetheless, retrieval is a powerful and efficient approach because the external store can be arbitrarily large and kept up-to-date, sidestepping the need to retrain the model for new knowledge. Many see RAG as a practical form of LLM “long-term memory” that complements the frozen knowledge in the model’s weights[20].
Efficacy: Empirical results show clear benefits. As noted, DeepMind’s RETRO demonstrated massive gains in perplexity and factual accuracy by leveraging a database[21][15]. In the context of open-domain QA, retrieval-augmented models like RAG and Atlas topped leaderboards by answering questions with supporting evidence, something closed models struggled with. For conversation, user reports and studies indicate that chatbots with memory (via retrieval) produce more consistent and personalized responses than those that forget past interactions[22]. A 2023 study on long dialogues found that pure retrieval-based memory can falter if the retriever misses something, but when combined with other techniques (like summaries), it significantly improved coherence over very long chats[23][24]. Overall, retrieval is one of the most widely used “long-term memory” techniques in real systems today, due to its simplicity and the large performance boosts observed on knowledge-focused tasks.
Summarization and Context Compression (“Context Scaffolding”)
A different tactic to handle long-term interactions is context management via summarization. Instead of storing everything and retrieving raw transcripts, the idea is to compress old information into shorter summaries that can be kept in the prompt. This is sometimes called “context scaffolding” – building a reduced representation of prior content that fits within the context window.
Conversation Summaries: In multi-turn conversations, a common approach is to summarize earlier turns once the dialogue gets too long. For example, after 50 messages, the system might replace the full chat history with a concise summary of what has been discussed, then continue the conversation with that summary as context. This way, the model retains the important points without needing every utterance word-for-word. Many chat frameworks implement this: the LangChain library’s ConversationSummaryMemory is one such component that periodically compresses the dialogue[25][26]. This helps prevent context overflow and reduces the chance of inconsistency that comes from losing older messages entirely.
Recursive or Iterative Summarization: A notable 2023 method, Recursively Summarizing, showed that having the LLM continually generate and update summaries (“memory”) of the dialogue leads to much better long-term consistency[27][22]. In this approach, the LLM is prompted at intervals to summarize recent conversation into a memory slot. As the dialogue continues, it may produce a new summary that combines the previous summary with the latest exchanges, and so on (hence “recursive”). The final response is generated with the latest summary as an aid. This method enabled chatbots to handle dialogues spanning hundreds of turns by recursive compression. Experiments found that it yields more consistent responses in long-term conversations compared to both naive long-context models and simple retrieval[22]. In fact, with just a few examples of how to summarize given in the prompt, even GPT-3.5 showed significant improvement in maintaining context over long dialogues[28][23]. Human evaluators preferred the consistency of the summary-augmented chats, and automatic metrics (like BLEU for dialogue) improved slightly (+3% on one benchmark) when using the memory summaries[29].
Avoiding Drift and Staleness: One challenge with summaries is that they can become too high-level or outdated. If you summarize too aggressively, the model might miss specific details later on (“Did we mention the user’s cat’s name? It got lost in the summary.”). The recursive strategy addresses this by updating summaries rather than keeping a single static summary. Also, some systems use multiple layers of summary: e.g., keep a very short synopsis of the entire conversation plus a more detailed summary of the recent topic, etc. This resembles how a person might recall a general gist plus some recent specifics. It was observed that without iterative updates, a fixed summary can harm response quality if the conversation shifts – the model might rely on outdated info[30][31]. Therefore, careful design is needed so that summaries help rather than confuse the model.
Context Truncation and Focus: Simpler forms of context management include just forgetting the oldest turns (truncate history) or using rules to only keep the last N user messages. This doesn’t truly solve long-term memory, but it’s a baseline many systems used early on. More advanced is focus-based pruning: e.g., keep only the parts of context relevant to the current topic (determined by some heuristic or model). The Anthropic “context distillation” blog (2023) suggests selectively dropping low-relevance tokens to fight context rot[32] – essentially a form of summarization or filtering to keep the model focused on what matters. These techniques can be seen as primitive scaffolding.
Efficacy: Summarization can dramatically extend the effective dialogue length a model can handle. The recursive summary paper reported qualitatively that even GPT-3.5, with an 8k window, could carry on a conversation spanning 20+ turns about a user’s persona without contradictions, whereas vanilla GPT-3.5 would forget earlier details (like what the user’s job was)[33][34]. In practice, many users have found that providing a summary of previous sessions to ChatGPT can help it simulate long-term memory. The downside is that a summary is only as good as what it captures – if something was omitted, the model has truly forgotten it. Still, combined with retrieval (“you can retrieve specific facts, and summarize the rest”), summarization-based memory is a powerful tool. It complements large context windows (even a model with 100k context benefits from summarizing irrelevant parts) and pairs with retrieval (one study showed using both yielded the best long-dialogue performance)[23][24].
Specialized Memory Architectures and Modules
Beyond straightforward retrieval or summarization, researchers have explored dedicated memory modules or architectural changes to imbue LLMs with something like a working memory or long-term memory component. These often draw inspiration from how humans organize memory (episodic vs semantic) or from computer systems (caches, graphs, etc.). Here are a few notable paradigms:
Key-Value Memory Networks: Originally pioneered around 2016–2018, memory-augmented neural networks introduced an external key-value store that the model could write to and read from (using attention mechanisms). Modern LLM agents sometimes use a similar idea for episodic memory: they store (key = a situation or query, value = an LLM-generated response or summary of that situation). When a new query comes, the agent finds the closest key and uses the corresponding value (memory) to inform its answer. This is analogous to a Q&A flashcard system[35][36]. One can imagine an agent accumulating a table of “When in context X, here’s what to do” and consulting it. However, pure key-value episodic memory can bloat in size and is only as good as the coverage of keys – it may not generalize well beyond exact matches[37][38]. It’s useful in task-oriented dialogue where similar situations recur and can be memorized.
Hierarchical Memory & Graphs: Human memory seems to organize experiences hierarchically (we form summaries, then summaries of summaries, etc.) and by relationships (a semantic network of concepts). Emulating this, some approaches build a hierarchical memory – for example, daily summaries feed into weekly summaries, etc., forming layers of abstraction (this was used in certain long-term chatbots and storytelling systems). Others maintain a knowledge graph as memory: entities and their relationships discovered during conversation are stored in a graph database. This graph can be queried to answer questions like “Who is Alice’s boss?” (if that relation was mentioned before). Graph-structured memory helps with consistency and reasoning about relationships. For instance, if an agent reads a story, it could store a graph of characters and their attributes; later, it can consult the graph to avoid plot holes. Microsoft’s 2023 Generative Agents paper used a form of structured memory: each agent records observations (facts) with timestamps, and has a mechanism to reflect on them and infer higher-level insights (forming new memory chunks)[39]. This roughly mimics forming semantic memory from episodic experiences. The agents then use relevance queries to retrieve memories that influence their behavior. This yielded impressively consistent character behavior over time (agents remembered past interactions and formed plans) in the sandbox simulation, though it wasn’t a numeric benchmark per se.
Hybrid Memory Systems: The most cutting-edge thinking in 2024–2025 is that no single memory form suffices – instead, systems combine multiple layers: e.g. a short-term buffer, a vector semantic memory, a structured knowledge graph, plus the model’s built-in parametric memory[40]. A Hybrid memory might store recent conversation verbatim, older facts in a graph, and use the model weights for core world knowledge. By layering memory this way, one can apply the right tool for each type of information. Indeed, a trend in 2025 is that advanced LLM agent frameworks incorporate a “memory hierarchy”[41]:
A fast, limited buffer for immediate context (like the recent conversation window).
A vector store for episodic recall (important past interactions, retrieved by similarity).
A knowledge base or graph for facts and relations (allowing logical queries).
Policies for memory updating and forgetting (e.g. using a “forgetting curve” to gradually decay seldom-used memories, or merging redundant memories)[42].

Figure: Example memory architecture (from the MemGPT system) where a fixed-context LLM is augmented with multi-level memory. The model’s prompt is divided into a main context (like RAM: system instructions + a working area + a FIFO queue of recent messages) and an external context (like disk storage). Special functions allow the LLM to write out old information to an archive or recall information back into the prompt when needed[43][44]. This design lets the LLM manage an effectively unbounded conversation by paging data in and out of its limited context.
One concrete example of a hybrid memory is MemGPT (Memory GPT)[45][46], which treats the LLM as an “operating system” managing its memory. MemGPT gives the LLM tools (function calls) to explicitly move data between a short-term context and long-term storage[47][44]. During a conversation, if the context is about to overflow, MemGPT’s policy inserts a “memory pressure” signal and the LLM can decide to save important info to an external store (analogous to writing to disk)[48][49]. Later, if the user references something from far back, the LLM can retrieve it via a function call to the external memory and bring it back into the context[43][50]. This autonomous memory management was shown to outperform vanilla LLMs on tasks like analyzing long documents and multi-session chats[46][51]. Essentially, MemGPT gives the LLM agency to handle its limited context, creating the illusion of an infinite context by clever shuffling of information in and out.
Efficacy: Specialized memory architectures are still in early research, so we have fewer standardized metrics. However, individual studies report substantial gains in tasks that require long-term consistency: - LongMem (2023) – a model that adds a decoupled memory bank and a “Side network” to a frozen LLM – achieved state-of-the-art results on ChapterBreak, a long-context benchmark, with 40.5% accuracy[52]. Notably, this beat even GPT-3 (which had 313× more parameters) on identifying chapter boundaries in books[52]. LongMem also improved accuracy by ~8 points on a suite of few-shot NLP tasks when a large number of examples had to be “remembered” from the memory bank[53]. These gains show the benefit of a dedicated memory module for long texts: LongMem could model book-length dependencies better than standard transformers[54][55]. - Generative Agents (2023) – in a simulation of 25 AI characters living in a neighborhood, a structured memory (with importance scoring and periodic reflection) enabled agents to remember events days later and synthesize those into new plans (e.g. one agent remembered a conversation and two days later initiated a party based on it). While qualitative, this demonstrated believable long-term behavior, suggesting the memory system was effective[39]. - MemVerse (2025) – this is a proposed unified memory framework that explicitly combines fast parametric memory with a structured long-term store[56]. MemVerse uses a hierarchical knowledge graph to accumulate experiences (text, images, etc.) and periodically distills the important pieces into the model’s weights[57][58]. The LLM Watch newsletter reports that MemVerse significantly improved an agent’s continuous learning, preventing catastrophic forgetting and boosting performance on long-horizon tasks (for instance, a multimodal agent with MemVerse had an ~8.4% higher accuracy on long video reasoning than prior methods)[59][60]. This kind of result indicates that hybrid memory+learning systems can yield substantial improvements in sustained reasoning.
In summary, specialized memory architectures aim to make LLMs more like humans: retaining a lifetime of experiences in structured form, and consolidating the most important knowledge internally over time. They are complex to implement (needing orchestration and sometimes training overhead), but they promise more scalable long-term memory than any single technique alone. We expect future LLM agents to incorporate these multi-tier memory systems for truly lifelong learning and interaction[61][62].
Updating Model Weights for Long-Term Memory (Parametric Memory)
The approaches above mostly keep long-term information outside the model (in context or external stores). Another way to give an LLM long-term memory is to write the information into the model’s parameters. After all, pretraining already did this for a vast corpus (the model’s weights encode a lot of world knowledge). The question is whether we can continually update those weights with new data or new interactions so the model “remembers” it next time without being explicitly reminded in the prompt.
Fine-Tuning and Continual Learning: The simplest method is to fine-tune the model on new information. For example, if you want a deployed LLM to learn about events from 2023, you could periodically fine-tune it on news articles from 2023. This bakes the new knowledge into its weights (parametric memory), so it can recall facts from 2023 without an external lookup. However, a big challenge here is catastrophic forgetting: when you train on new data, the model might distort or lose some of its previously learned knowledge[63]. Researchers in continual learning for LLMs are developing techniques to avoid forgetting, such as regularization that preserves important weights, or interleaving old data (replay) when learning new data[64]. One 2023 survey noted that continually fine-tuning large models remains hard, but it’s an active area[65]. Some have proposed online learning setups where an LLM is regularly fed a stream of new data (say, daily) with carefully controlled updates. The key is to make these updates efficient and safe – ideally without needing a full re-training.
Memory Editing Methods: A more targeted approach than full fine-tuning is model editing – directly modifying the weights to implant a specific piece of knowledge. For instance, if the model says “The CEO of X company is Alice” but it changed to Bob, you can perform a surgical edit to its memory. Recent techniques like ROME, MEND, and MEMIT (all 2022–2023) allow one to locate which internal weights correspond to a factual association and change them to store a new fact. Impressively, MEMIT demonstrated the ability to mass-edit thousands of facts at once in a transformer, with minimal side effects on unrelated knowledge[66]. In other words, you could update a model’s factual memory base in bulk (like updating an internal knowledge graph). These methods treat the model’s weights as addressable memory cells for specific info. By applying them, one could give an LLM long-term memory updates without retraining from scratch. For example, OpenAI could use such a method to inject new world facts into GPT-4 periodically, ensuring its responses stay up-to-date.
Learnt Memory Units / Fast Weights: Some research revisits the idea of fast weights – having the model dynamically adjust a portion of its weights during inference to store new information. This is analogous to how a brain might form a new synaptic pattern when learning something in the moment. There have been experiments where an LLM has a small writable cache of parameters that it can modify as it processes new text, effectively learning inside its forward pass. This is quite experimental, but if perfected, it means the LLM could truly learn from a conversation (updating itself by the end of the chat). As of 2025, most deployed systems don’t have this ability; they rely on context or external memory instead of changing the model on the fly. But conceptually, this would be the ultimate long-term memory: the model would simply absorb new knowledge into itself and recall it forever after.
Efficacy: Incorporating knowledge into model weights can make recall more efficient and reliable (no need to retrieve – the model just knows it). For well-defined tasks, this works extremely well. For instance, OpenAI fine-tuned GPT-3 into instruction-following GPT-3.5 by training on conversation data (so it “remembers” how to follow instructions without being explicitly prompted every time). In the academic realm, the MemVerse framework described earlier found that periodically distilling external memory into the model led to a virtuous cycle: the model gradually improved its internal knowledge and needed to retrieve less from outside[67][58]. This prevented the model from forgetting important facts over time, and avoided simply growing an unmanageable external log. In one case, a multimodal agent with such periodic weight updates was able to continuously learn from a stream of experiences (images + text) and later answer questions that required integrating those experiences, outperforming a baseline that didn’t update the model. On the flip side, if done improperly, updating weights can degrade performance. A 2023 study on editing LLMs warns that naive editing can have cascading effects, accidentally altering behavior on unrelated queries[68]. Techniques like MEMIT mitigate this by being very specific, and showed minimal impact on unrelated facts even after editing 5,000 facts in GPT-J[66].
In practice, major AI providers currently lean on retrieval rather than live weight-updating for model memory (because safe continual learning is hard). But the line is blurring: for example, OpenAI’s plug-in system or Microsoft’s Bing augment GPT-4 with external data (retrieval), while OpenAI’s upcoming GPTs (personalized agents) might fine-tune on your data with your permission, effectively writing a form of long-term memory into a dedicated model instance[69]. Meanwhile, enterprise solutions sometimes do domain adaptation fine-tunes – e.g. fine-tuning an LLM on a company’s documents so that it “knows” them without needing a database lookup. This is parametric long-term memory at work.

Conclusion
In summary, LLMs are being augmented with long-term memory through multiple complementary approaches:
Longer context windows and efficient transformers give immediate but shallow long-term memory (great for ingesting big inputs, but with diminishing returns beyond a point)[32][11].
External memory via retrieval provides a massive, updatable knowledge base that LLMs can query, dramatically improving factual accuracy and allowing essentially unlimited dialogue history[15][17].
Context management techniques like summarization and hierarchical memory help LLMs maintain coherence by compressing and organizing information, as seen in improved long-dialogue consistency[22][28].
Advanced memory architectures (key-value stores, graphs, hybrid systems) enable structured storage and selective recall, which can yield more human-like recall of both specific events and general knowledge[56][70].
Incorporating memory into model weights through continual learning or editing ensures the model itself grows its knowledge and can recall things without any prompt or tool – though this is the hardest to do non-destructively, research is making strides (e.g. thousands of facts edited into a model with minimal side-effects)[66].
Each approach has its pros and cons. External memory is flexible and keeps the model size fixed, but relies on good retrieval and doesn’t truly “understand” long-term context – the model might still repeat itself or contradict itself if the memory isn’t integrated well. Summarization reduces load but risks omitting details. Long contexts let the model directly reason over lots of info, but are computationally heavy and can suffer context dilution[9][32]. Writing to weights makes knowledge permanent and fast to access, but can cause forgetting or require complex interventions to avoid model collapse.
Notably, real-world large-scale systems are now combining these techniques. ChatGPT’s new memory feature combines explicit user-taught facts (which is like writing to a personal knowledge base) with retrieval of past chat history[18][19]. Emerging agent frameworks (AutoGPT, etc.) use vector databases for tools and past results, and sometimes prompt the model to summarize its plan or reflect (storing that as pseudo-memory). The cutting-edge MemVerse concept actually bridges parametric and external memory – periodically fine-tuning the model on its accumulated experiences[56]. This hybrid could yield the best of both worlds: a model that learns continuously (so important knowledge is internalized for quick access) while also maintaining an explicit memory store for details and transparency[71][62].
As evaluations go, different methods shine on different metrics. Retrieval and long context excel on factual recall tests (e.g. answering questions from a long document). Summarization-based memory excels in conversational consistency and reducing contradictions in long chats[22]. Memory architectures and hybrid learning approaches tend to show their value on long-horizon or lifelong tasks, like reading an entire book and then answering questions throughout it, or an agent that must carry out a project over many steps. For example, World models with episodic memory reported ~8% higher success on long video comprehension[60], and multi-agent educational tutors with memory more than doubled performance vs. stateless baselines (from score 5.32 to 9.23 in one study)[72]. Such improvements underscore how crucial memory is for complex reasoning.
Finally, it’s worth noting that humans have multiple memory systems (short-term scratchpad, long-term semantic, episodic memories, etc.), and it appears LLM-based AI will similarly require a combination of systems to truly operate over extended timescales. No single benchmark tells the whole story, but across the board, adding these memory mechanisms has made LLMs more powerful, enabling things like week-long conversations, interactive fiction that recalls past chapters, personal assistants that remember your preferences, and agents that learn from experience. The area of long-term memory for LLMs is evolving rapidly, and the most successful approaches so far use a blend of context extension, external memory, and model updates to give us glimpses of “lifelong learning” in AI[62]. Each method contributes to better performance, and together they are pushing LLMs closer to the fluid memory of a human collaborator.
Sources: This survey is based on recent papers and reports, including retrieval-augmented models by DeepMind[15], long-context research from Google/DeepMind[11], Anthropic’s insights on context limits[9], dialogue memory research in 2023[22], and advanced memory frameworks like LongMem[52], MemGPT[43], and MemVerse[56], among others. These illustrate the spectrum of approaches and their outcomes in improving LLM memory and consistency. The field has accelerated from early prototypes to sophisticated multi-memory systems in just the last two years[59][61], indicating that effective long-term memory for LLMs is on the horizon, powered by the convergence of these techniques.

[1] [20] [22] [23] [24] [27] [28] [29] [30] [31] [33] [34] Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models
https://arxiv.org/html/2308.15022v3
[2] [3] [4] [10] [11] Introducing Gemini 1.5, Google's next-generation AI model
https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/
[5] [2304.11062] Scaling Transformer to 1M tokens and beyond with RMT
https://arxiv.org/abs/2304.11062
[6] [7] [8] [9] [12] [32] Context rot: the emerging challenge that could hold back LLM progress
https://www.understandingai.org/p/context-rot-the-emerging-challenge
[13] [14] [17] [19] AI memory explained: what Perplexity, ChatGPT, Pieces, and Claude remember (and forget)
https://pieces.app/blog/types-of-ai-memory
[15] [16] [21]  Improving language models by retrieving from trillions of tokens - Google DeepMind 
https://deepmind.google/blog/improving-language-models-by-retrieving-from-trillions-of-tokens/
[18] [69] Memory and new controls for ChatGPT | OpenAI
https://openai.com/index/memory-and-new-controls-for-chatgpt/
[25] Short-term memory - Docs by LangChain
https://docs.langchain.com/oss/python/langchain/short-term-memory
[26] Conversation Summary Memory in LangChain - GeeksforGeeks
https://www.geeksforgeeks.org/artificial-intelligence/conversation-summary-memory-in-langchain/
[35] [36] [37] [38] [40] [41] [42] [70] Long-Term Memory for LLMs: 2023 – 2025 – Champaign Magazine
https://champaignmagazine.com/2025/10/14/long-term-memory-for-llms-2023-2025/
[39] Explore Generative AI Agents that can Remember, Reflect, and Plan ...
https://towardsai.net/p/l/explore-generative-ai-agents-that-can-remember-reflect-and-plan-actions
[43] [44] [45] [46] [47] [48] [49] [50] [51] [2310.08560] MemGPT: Towards LLMs as Operating Systems
https://ar5iv.labs.arxiv.org/html/2310.08560
[52] [53] [54] [55] Augmenting Language Models with Long-Term Memory
https://arxiv.org/html/2306.07174
[56] [57] [58] [59] [60] [61] [62] [67] [71] [72] AI Agents of the Week: Papers You Should Know About
https://www.llmwatch.com/p/ai-agents-of-the-week-papers-you-b5b
[63] [64] Introducing Nested Learning: A new ML paradigm for continual ...
https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/
[65] Continual Learning of Large Language Models - ACM Digital Library
https://dl.acm.org/doi/10.1145/3735633
[66] Mass Editing Memory in a Transformer
https://memit.baulab.info/
[68] Stable Knowledge Editing in Large Language Models - arXiv
https://arxiv.org/html/2402.13048v1